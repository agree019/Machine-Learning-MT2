{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c344726",
   "metadata": {},
   "source": [
    "# Finding the Mass of a W-boson from Four-momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad98fd2",
   "metadata": {},
   "source": [
    "Given the 4 momentum of the W-boson, I am hoping for the neural network to learn the formula $$M^2 = E^2 - p^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5bdfb",
   "metadata": {},
   "source": [
    "This time, instead of making it from scratch, I will try doing it using TensorFlow, which I've installed on my computer.\n",
    "\n",
    "By the way, I've figured out what was wrong with my neural network from scratch code. I used the sigmoid activation function, which apparently guarentees a value between 0 and 1. This is why it kept being stuck at 1 no matter how many nodes we added to it. As such, this time I'll be using the ReLU activation function.\n",
    "\n",
    "Also FYI, while I did look at Adam's notebook on this project as well as the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/keras/classification) on the official site, I primarily used this [YouTube video](https://www.youtube.com/watch?v=Edhv7-4t0lc&list=PLqnslRFeH2Uqfv1Vz3DqeQfy0w20ldbaV&index=3) to guide me on constructing the neural network.\n",
    "\n",
    "PID of W-boson = -24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12118d1d",
   "metadata": {},
   "source": [
    "Inputting the data should be relatively similar. (Using 1TeVDecay.csv file)\n",
    "\n",
    "I assume that this meets [high-level guiding principle](https://github.com/agree019/Machine-Learning-MT2/blob/working/Nathan%20Meetings/9.28.22.pdf) #2 because if you wanted to get the 4 momentum squared or mass squared you could always just do np.square in front of the px/py/pz/e/m (although I suppose that's changing 5 lines instead of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f2af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Px         Py         Pz          E\n",
      "1      34.082959   0.447484  21.329689  40.209501\n",
      "16     18.124688 -34.795598   8.807158  40.209501\n",
      "34    -30.133966 -11.240563 -24.132920  40.209501\n",
      "52    -11.321488 -32.140145  21.345702  40.209501\n",
      "55    -24.034663 -31.920715  -4.495212  40.209501\n",
      "...          ...        ...        ...        ...\n",
      "29932 -11.878805  15.684833 -35.066850  40.209501\n",
      "29935 -22.737105 -24.291624  22.577534  40.209501\n",
      "29959  30.230409  -1.883528  26.445769  40.209501\n",
      "29962  25.366209   4.016472 -30.939091  40.209501\n",
      "29992 -18.683008 -20.249168 -29.286864  40.209501\n",
      "\n",
      "[2468 rows x 4 columns]\n",
      "2468\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "TeVData = pd.read_csv(\"100TeVDecay.csv\")\n",
    "dataInput = TeVData[TeVData.PID == -11][['Px', 'Py', 'Pz', 'E']]\n",
    "print(dataInput)\n",
    "dataOutput = TeVData[TeVData.PID == -11][['M']]\n",
    "print(len(dataInput))\n",
    "\n",
    "# px = np.asarray(TeVData[TeVData.PID == -24]['Px'])\n",
    "# py = np.asarray(TeVData[TeVData.PID == -24]['Py'])\n",
    "# pz = np.asarray(TeVData[TeVData.PID == -24]['Pz'])\n",
    "# e = np.asarray(TeVData[TeVData.PID == -24]['E'])\n",
    "# m = np.asarray(TeVData[TeVData.PID == -24]['M'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataInput, dataOutput, test_size=0.25, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2da4a8",
   "metadata": {},
   "source": [
    "The next part will be importing TensorFlow, setting up the network topology (input nodes, # of hidden layers, # of hidden nodes per layer, output nodes), the loss-function, the optimizer (our learning rate, not sure if you can put momentum in Adam optimizer), and our metrics (what we want the program to print out while epochs are running, like % of datasets program got correct during training/testing, full list on [TensorFlow website](https://www.tensorflow.org/api_docs/python/tf/keras/metrics))\n",
    "\n",
    "Hopefully this part covers [high-level guiding principle](https://github.com/agree019/Machine-Learning-MT2/blob/working/Nathan%20Meetings/9.28.22.pdf) #4 (changing network \"topology\" easily), principle #1 also covered because we are using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1490b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#model\n",
    "layers = []\n",
    "model = tf.keras.Sequential()\n",
    "layers.append(keras.layers.Dense(6, activation = 'relu', input_dim = 4)) #number of input nodes\n",
    "model.add(layers[0])\n",
    "for i in range(1): #number of hidden layers - 1, can be changed\n",
    "    layers.append(keras.layers.Dense(6, activation = 'relu'))\n",
    "    model.add(layers[i + 1])\n",
    "    #Note about hidden layers: This might not be able to be changed in 1 line if we want a different number of nodes for each\n",
    "    #hidden layer (for example hidden layer 1 has 5 nodes, hidden layer 2 has 12 nodes, hidden layer 3 has 15 nodes)\n",
    "    #This for loop only really works if the same # of nodes are in each hidden layer (in this case 6 for each)\n",
    "    #I'll look into this to see if it can be changed but according to the video + other resources I'm not quite sure if this\n",
    "    #is something you can just do in one line\n",
    "layers.append(keras.layers.Dense(6, activation = 'relu'))\n",
    "model.add(layers[len(layers) - 1])\n",
    "#num. of output nodes I assume output layer size is 1 because we are just printing out the \n",
    "#mass of the W-boson, but output nodes can be changed\n",
    "print(model.summary()) #check if topology working correctly\n",
    "\n",
    "#loss and optimizer\n",
    "loss = keras.losses.MeanSquaredError() #using MSE for error calculation\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.1) #put in learning rate, using 0.1\n",
    "metrics = [\"accuracy\"] #not sure if I need to include other metrics besides accuracy but this can be changed\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics) #configure model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4459a1",
   "metadata": {},
   "source": [
    "After that, we will be training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d0fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 139ms/epoch - 2ms/step\n",
      "Epoch 2/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 140ms/epoch - 2ms/step\n",
      "Epoch 3/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 154ms/epoch - 3ms/step\n",
      "Epoch 4/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 152ms/epoch - 3ms/step\n",
      "Epoch 5/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 144ms/epoch - 2ms/step\n",
      "Epoch 6/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 150ms/epoch - 3ms/step\n",
      "Epoch 7/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 140ms/epoch - 2ms/step\n",
      "Epoch 8/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 150ms/epoch - 3ms/step\n",
      "Epoch 9/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 171ms/epoch - 3ms/step\n",
      "Epoch 10/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 156ms/epoch - 3ms/step\n",
      "Epoch 11/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 192ms/epoch - 3ms/step\n",
      "Epoch 12/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 137ms/epoch - 2ms/step\n",
      "Epoch 13/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 148ms/epoch - 3ms/step\n",
      "Epoch 14/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 153ms/epoch - 3ms/step\n",
      "Epoch 15/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 173ms/epoch - 3ms/step\n",
      "Epoch 16/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 166ms/epoch - 3ms/step\n",
      "Epoch 17/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 152ms/epoch - 3ms/step\n",
      "Epoch 18/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 151ms/epoch - 3ms/step\n",
      "Epoch 19/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 152ms/epoch - 3ms/step\n",
      "Epoch 20/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 150ms/epoch - 3ms/step\n",
      "Epoch 21/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 179ms/epoch - 3ms/step\n",
      "Epoch 22/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 190ms/epoch - 3ms/step\n",
      "Epoch 23/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 142ms/epoch - 2ms/step\n",
      "Epoch 24/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 153ms/epoch - 3ms/step\n",
      "Epoch 25/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 175ms/epoch - 3ms/step\n",
      "Epoch 26/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 201ms/epoch - 3ms/step\n",
      "Epoch 27/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 161ms/epoch - 3ms/step\n",
      "Epoch 28/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 147ms/epoch - 3ms/step\n",
      "Epoch 29/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 143ms/epoch - 2ms/step\n",
      "Epoch 30/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 152ms/epoch - 3ms/step\n",
      "Epoch 31/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 213ms/epoch - 4ms/step\n",
      "Epoch 32/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 143ms/epoch - 2ms/step\n",
      "Epoch 33/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 137ms/epoch - 2ms/step\n",
      "Epoch 34/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 197ms/epoch - 3ms/step\n",
      "Epoch 35/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 155ms/epoch - 3ms/step\n",
      "Epoch 36/36\n",
      "58/58 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 184ms/epoch - 3ms/step\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='dense_2/Relu:0', description=\"created by layer 'dense_2'\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#training data\n",
    "batch_size = 32 #batch size\n",
    "epochs = 36 #epochs, can be adjusted\n",
    "model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, shuffle = True, verbose = 2)\n",
    "#not sure what verbose is, but I think it makes the output more specific when printing out errors\n",
    "\n",
    "# print out weights/layers\n",
    "# for i in layers:\n",
    "#     print(i.get_weights())\n",
    "print(layers[len(layers) - 1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d34f1",
   "metadata": {},
   "source": [
    "This part is testing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f72f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 312ms/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size = batch_size, verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
